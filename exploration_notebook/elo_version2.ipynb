{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c989abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:39:02.330104Z",
     "start_time": "2024-06-12T09:39:01.915584Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e634f",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59016336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:39:02.533863Z",
     "start_time": "2024-06-12T09:39:02.330505Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test =  pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b77bb57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:39:02.542904Z",
     "start_time": "2024-06-12T09:39:02.535668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201917, 6), (123623, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ff42a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:39:02.553241Z",
     "start_time": "2024-06-12T09:39:02.543938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "2            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "3            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "4            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  \n",
       "0 -0.820283  \n",
       "1  0.392913  \n",
       "2  0.688056  \n",
       "3  0.142495  \n",
       "4 -0.159749  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ed058b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:39:02.562204Z",
     "start_time": "2024-06-12T09:39:02.554907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04</td>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08</td>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12</td>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12</td>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3\n",
       "0            2017-04  C_ID_0ab67a22ab          3          3          1\n",
       "1            2017-01  C_ID_130fd0cbdd          2          3          0\n",
       "2            2017-08  C_ID_b709037bc5          5          1          1\n",
       "3            2017-12  C_ID_d27d835a9f          2          1          0\n",
       "4            2015-12  C_ID_2b5e3df5c2          5          1          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418e06e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-12T09:39:01.912Z"
    }
   },
   "outputs": [],
   "source": [
    "history_transaction = pd.read_csv('./data/historical_transactions.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_transaction.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d38f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b02a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_transaction[history_transaction['card_id'] == 'C_ID_92a2005557']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant = pd.read_csv('./data/merchants.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_transaction.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf198386",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant[merchant['merchant_id'] == 'M_ID_e020e9b302']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del history_transaction, merchant\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5717e2",
   "metadata": {},
   "source": [
    "# Filter features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocess/train.csv')\n",
    "test = pd.read_csv('preprocess/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396cb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e345fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - np.count_nonzero(train) / train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4edec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()\n",
    "features.remove(\"card_id\")\n",
    "features.remove(\"target\")\n",
    "featureSelect = features[:]\n",
    "\n",
    "corr = []\n",
    "for fea in featureSelect:\n",
    "    corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "feature_select = ['card_id'] + se[:300].index.tolist()\n",
    "\n",
    "train_RF = train[feature_select + ['target']]\n",
    "test_RF = test[feature_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f477b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_pearson(train, test):\n",
    "    \"\"\"\n",
    "    use pearson to filter the features\n",
    "    :param train:training data\n",
    "    :param test:testing data\n",
    "    :return: training and testing data after filtering\n",
    "    \"\"\"\n",
    "    print('feature_select...')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "    featureSelect = features[:]\n",
    "\n",
    "    # Remove features with missing value exceeding 0.99\n",
    "    for fea in features:\n",
    "        if train[fea].isnull().sum() / train.shape[0] >= 0.99:\n",
    "            featureSelect.remove(fea)\n",
    "\n",
    "    # do pearson corr calculation\n",
    "    corr = []\n",
    "    for fea in featureSelect:\n",
    "        corr.append(abs(train[[fea, 'target']].fillna(0).corr().values[0][1]))\n",
    "\n",
    "    # choose top300 features to do model training\n",
    "    se = pd.Series(corr, index=featureSelect).sort_values(ascending=False)\n",
    "    feature_select = ['card_id'] + se[:300].index.tolist()\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83732fa",
   "metadata": {},
   "source": [
    "# Random forest model training & Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11002d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_search(train):\n",
    "    \"\"\"\n",
    "    grid search hypertuning\n",
    "    :param train:training set\n",
    "    :return:result of grid search training\n",
    "    \"\"\"\n",
    "    # Step 1.create params of grid search\n",
    "    print('param_grid_search')\n",
    "    features = train.columns.tolist()\n",
    "    features.remove(\"card_id\")\n",
    "    features.remove(\"target\")\n",
    "    parameter_space = {\n",
    "        \"n_estimators\": [81], \n",
    "        \"min_samples_leaf\": [31],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"max_depth\": [10],\n",
    "        \"max_features\": [80]\n",
    "    }\n",
    "    \n",
    "    # Step 2.execute grid search\n",
    "    print(\"Tuning hyper-parameters for mse\")\n",
    "\n",
    "    clf = RandomForestRegressor(\n",
    "        criterion=\"mse\",\n",
    "        n_jobs=15,\n",
    "        random_state=22)\n",
    "\n",
    "    grid = GridSearchCV(clf, parameter_space, cv=2, scoring=\"neg_mean_squared_error\")\n",
    "    grid.fit(train[features].values, train['target'].values)\n",
    "    \n",
    "    # Step 3.output results of grid search\n",
    "    print(\"best_params_:\")\n",
    "    print(grid.best_params_)\n",
    "    means = grid.cv_results_[\"mean_test_score\"]\n",
    "    stds = grid.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    for mean, std, params in zip(means, stds, grid.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = param_grid_search(train_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b257b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d350f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = grid.best_estimator_.predict(test[features])\n",
    "test[['card_id', 'target']].to_csv(\"result/submission_randomforest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fbeae",
   "metadata": {},
   "source": [
    "# Wrapper feature filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4af1632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:02:56.074087Z",
     "start_time": "2024-06-12T11:02:56.070368Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_wrapper(train, test):\n",
    "    \"\"\"\n",
    "    lgm features filtering\n",
    "    :param train:training dataset\n",
    "    :param test:testing dataset\n",
    "    :return:training and testing dataset after filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    # Part 1.delete column 'id' and 'target'\n",
    "    print('feature_select_wrapper...')\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "\n",
    "    # Step 2.configuring lgm\n",
    "\n",
    "    params_initial = {\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'boosting': 'gbdt',\n",
    "        'min_child_samples': 20,\n",
    "        'bagging_seed': 2020,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': 0.7,\n",
    "        'max_depth': -1,\n",
    "        'metric': 'rmse',\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 1,\n",
    "        'objective': 'regression'\n",
    "    }\n",
    "\n",
    "    ESR = 30\n",
    "\n",
    "    NBR = 10000\n",
    "\n",
    "    VBE = 50\n",
    "    \n",
    "    # Part 3.kfold validation\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "\n",
    "    fse = pd.Series(0, index=features)\n",
    "    \n",
    "    for train_part_index, eval_index in kf.split(train[features], train[label]):\n",
    "\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "\n",
    "        eval = lgb.Dataset(train[features].loc[eval_index],\n",
    "                           train[label].loc[eval_index])\n",
    "\n",
    "        bst = lgb.train(params_initial, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, eval],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        early_stopping_rounds=ESR, verbose_eval=VBE)\n",
    "\n",
    "        fse += pd.Series(bst.feature_importance(), features)\n",
    "    \n",
    "    # Part 4.choose top300 features\n",
    "    feature_select = ['card_id'] + fse.sort_values(ascending=False).index.tolist()[:300]\n",
    "    print('done')\n",
    "    return train[feature_select + ['target']], test[feature_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60030a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LGBM, test_LGBM = feature_select_wrapper(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LGBM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe254900",
   "metadata": {},
   "source": [
    "# LightGBM model training & TPE optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_append(params):\n",
    "    \"\"\"\n",
    "    Dynamic callback parameter function，params as dict\n",
    "    :param params:lgb params dict\n",
    "    :return params:correct lgb params dict\n",
    "    \"\"\"\n",
    "    params['feature_pre_filter'] = False\n",
    "    params['objective'] = 'regression'\n",
    "    params['metric'] = 'rmse'\n",
    "    params['bagging_seed'] = 2020\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235782b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_hyperopt(train):\n",
    "    \"\"\"\n",
    "    model params searching and hypertuning\n",
    "    :param train:training dataset\n",
    "    :return params_best:lgb best params\n",
    "    \"\"\"\n",
    "\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    \n",
    "    train_data = lgb.Dataset(train[features], train[label])\n",
    "    \n",
    "    def hyperopt_objective(params):\n",
    "        \"\"\"\n",
    "        Input hyperparameters and output corresponding loss values\n",
    "        :param params:\n",
    "        :return:least rmse\n",
    "        \"\"\"\n",
    "        params = params_append(params)\n",
    "        print(params)\n",
    "\n",
    "        res = lgb.cv(params, train_data, 1000,\n",
    "                     nfold=2,\n",
    "                     stratified=False,\n",
    "                     shuffle=True,\n",
    "                     metrics='rmse',\n",
    "                     early_stopping_rounds=20,\n",
    "                     verbose_eval=False,\n",
    "                     show_stdv=False,\n",
    "                     seed=2020)\n",
    "        return min(res['rmse-mean']) # res is a dict\n",
    "\n",
    "    params_space = {\n",
    "        'learning_rate': hp.uniform('learning_rate', 1e-2, 5e-1),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n",
    "        'num_leaves': hp.choice('num_leaves', list(range(10, 300, 10))),\n",
    "        'reg_alpha': hp.randint('reg_alpha', 0, 10),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "        'bagging_freq': hp.randint('bagging_freq', 1, 10),\n",
    "        'min_child_samples': hp.choice('min_child_samples', list(range(1, 30, 5)))\n",
    "    }\n",
    "\n",
    "    params_best = fmin(\n",
    "        hyperopt_objective,\n",
    "        space=params_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=30,\n",
    "        rstate=RandomState(2020))\n",
    "\n",
    "    return params_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44323bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = param_hyperopt(train_LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be23a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ad5b3",
   "metadata": {},
   "source": [
    "### results of LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = params_append(best_clf)\n",
    "\n",
    "label = 'target'\n",
    "features = train_LGBM.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "lgb_train = lgb.Dataset(train_LGBM[features], train_LGBM[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716eab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = lgb.train(best_clf, lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a784b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.predict(train_LGBM[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(train_LGBM[label], bst.predict(train_LGBM[features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc847bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_LGBM['target'] = bst.predict(test_LGBM[features])\n",
    "test_LGBM[['card_id', 'target']].to_csv(\"result/submission_LGBM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_LGBM[['card_id', 'target']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4eb83",
   "metadata": {},
   "source": [
    " ### model prediction with combine cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label = 'target'\n",
    "    features = train.columns.tolist()\n",
    "    features.remove('card_id')\n",
    "    features.remove('target')\n",
    "    \n",
    "    params = params_append(params)\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    \n",
    "    prediction_test = 0\n",
    "    cv_score = []\n",
    "    prediction_train = pd.Series()\n",
    "    \n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    for train_part_index, eval_index in kf.split(train[features], train[label]):\n",
    "        train_part = lgb.Dataset(train[features].loc[train_part_index],\n",
    "                                 train[label].loc[train_part_index])\n",
    "        eval = lgb.Dataset(train[features].loc[eval_index],\n",
    "                           train[label].loc[eval_index])\n",
    "        bst = lgb.train(params, train_part, num_boost_round=NBR,\n",
    "                        valid_sets=[train_part, eval],\n",
    "                        valid_names=['train', 'valid'],\n",
    "                        early_stopping_rounds=ESR, verbose_eval=VBE)\n",
    "        prediction_test += bst.predict(test[features])\n",
    "        prediction_train = prediction_train.append(pd.Series(bst.predict(train[features].loc[eval_index]),\n",
    "                                                             index=eval_index))\n",
    "        eval_pre = bst.predict(train[features].loc[eval_index])\n",
    "        score = np.sqrt(mean_squared_error(train[label].loc[eval_index].values, eval_pre))\n",
    "        cv_score.append(score)\n",
    "        \n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(\"preprocess/train_lightgbm.csv\", index=False)\n",
    "    pd.Series(prediction_test / 5).to_csv(\"preprocess/test_lightgbm.csv\", index=False)\n",
    "    test['target'] = prediction_test / 5\n",
    "    test[['card_id', 'target']].to_csv(\"result/submission_lightgbm.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LGBM, test_LGBM = feature_select_wrapper(train, test)\n",
    "best_clf = param_hyperopt(train_LGBM)\n",
    "train_predict(train_LGBM, test_LGBM, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d65ac",
   "metadata": {},
   "source": [
    "### NLP features optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test =  pd.read_csv('data/test.csv')\n",
    "merchant = pd.read_csv('data/merchants.csv')\n",
    "new_transaction = pd.read_csv('data/new_merchant_transactions.csv')\n",
    "history_transaction = pd.read_csv('data/historical_transactions.csv')\n",
    "transaction = pd.concat([new_transaction, history_transaction], axis=0, ignore_index=True)\n",
    "del new_transaction\n",
    "del history_transaction\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_features = ['merchant_id', 'merchant_category_id', 'state_id', 'subsector_id', 'city_id']\n",
    "\n",
    "for co in nlp_features:\n",
    "    print(co)\n",
    "    transaction[co] = transaction[co].astype(str)\n",
    "    temp = transaction[transaction['month_lag']>=0].groupby(\"card_id\")[co].apply(list).apply(lambda x:' '.join(x)).reset_index()\n",
    "    temp.columns = ['card_id', co+'_new']\n",
    "    train = pd.merge(train, temp, how='left', on='card_id')\n",
    "    test = pd.merge(test, temp, how='left', on='card_id')\n",
    "\n",
    "    temp = transaction[transaction['month_lag']<0].groupby(\"card_id\")[co].apply(list).apply(lambda x:' '.join(x)).reset_index()\n",
    "    temp.columns = ['card_id', co+'_hist']\n",
    "    train = pd.merge(train, temp, how='left', on='card_id')\n",
    "    test = pd.merge(test, temp, how='left', on='card_id')\n",
    "\n",
    "    temp = transaction.groupby(\"card_id\")[co].apply(list).apply(lambda x:' '.join(x)).reset_index()\n",
    "    temp.columns = ['card_id', co+'_all']\n",
    "    train = pd.merge(train, temp, how='left', on='card_id').fillna(\"-1\")\n",
    "    test = pd.merge(test, temp, how='left', on='card_id').fillna(\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame()\n",
    "test_x = pd.DataFrame()\n",
    "\n",
    "cntv = CountVectorizer()\n",
    "tfv = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "\n",
    "vector_feature =[]\n",
    "for co in ['merchant_id', 'merchant_category_id', 'state_id', 'subsector_id', 'city_id']:\n",
    "    vector_feature.extend([co+'_new', co+'_hist', co+'_all'])\n",
    "    \n",
    "for feature in vector_feature:\n",
    "    print(feature)\n",
    "    cntv.fit([feature].append(test[feature]))\n",
    "    train_x = sparse.hstack((train_x, cntv.transform(train[feature]))).tocsr()\n",
    "    test_x = sparse.hstack((test_x, cntv.transform(test[feature]))).tocsr()\n",
    "    \n",
    "    tfv.fit(train[feature].append(test[feature]))\n",
    "    train_x = sparse.hstack((train_x, tfv.transform(train[feature]))).tocsr()\n",
    "    test_x = sparse.hstack((test_x, tfv.transform(test[feature]))).tocsr()\n",
    "    \n",
    "sparse.save_npz(\"preprocess/train_nlp.npz\", train_x)\n",
    "sparse.save_npz(\"preprocess/test_nlp.npz\", test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d90b4d",
   "metadata": {},
   "source": [
    "# XGBoost modeling training & optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03780d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import f_regression\n",
    "from numpy.random import RandomState\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocess/train.csv')\n",
    "test = pd.read_csv('preprocess/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.columns.tolist()\n",
    "features.remove('card_id')\n",
    "features.remove('target')\n",
    "\n",
    "train_x = sparse.load_npz(\"preprocess/train_nlp.npz\")\n",
    "test_x = sparse.load_npz(\"preprocess/test_nlp.npz\")\n",
    "\n",
    "train_x = sparse.hstack((train_x, train[features])).tocsr()\n",
    "test_x = sparse.hstack((test_x, test[features])).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_append(params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    params['objective'] = 'reg:squarederror'\n",
    "    params['eval_metric'] = 'rmse'\n",
    "    params[\"min_child_weight\"] = int(params[\"min_child_weight\"])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    return params\n",
    "\n",
    "def param_beyesian(train):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_y = pd.read_csv(\"data/train.csv\")['target']\n",
    "    sample_index = train_y.sample(frac=0.1, random_state=2020).index.tolist()\n",
    "    train_data = xgb.DMatrix(train.tocsr()[sample_index, :\n",
    "                             ], train_y.loc[sample_index].values, silent=True)\n",
    "    \n",
    "    def xgb_cv(colsample_bytree, subsample, min_child_weight, max_depth,\n",
    "               reg_alpha, eta,\n",
    "               reg_lambda):\n",
    "        \"\"\"\n",
    "\n",
    "        :param colsample_bytree:\n",
    "        :param subsample:\n",
    "        :param min_child_weight:\n",
    "        :param max_depth:\n",
    "        :param reg_alpha:\n",
    "        :param eta:\n",
    "        :param reg_lambda:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        params = {'objective': 'reg:squarederror',\n",
    "                  'early_stopping_round': 50,\n",
    "                  'eval_metric': 'rmse'}\n",
    "        params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "        params['subsample'] = max(min(subsample, 1), 0)\n",
    "        params[\"min_child_weight\"] = int(min_child_weight)\n",
    "        params['max_depth'] = int(max_depth)\n",
    "        params['eta'] = float(eta)\n",
    "        params['reg_alpha'] = max(reg_alpha, 0)\n",
    "        params['reg_lambda'] = max(reg_lambda, 0)\n",
    "        print(params)\n",
    "        cv_result = xgb.cv(params, train_data,\n",
    "                           num_boost_round=1000,\n",
    "                           nfold=2, seed=2,\n",
    "                           stratified=False,\n",
    "                           shuffle=True,\n",
    "                           early_stopping_rounds=30,\n",
    "                           verbose_eval=False)\n",
    "        return -min(cv_result['test-rmse-mean'])\n",
    "    \n",
    "    xgb_bo = BayesianOptimization(\n",
    "        xgb_cv,\n",
    "        {'colsample_bytree': (0.5, 1),\n",
    "         'subsample': (0.5, 1),\n",
    "         'min_child_weight': (1, 30),\n",
    "         'max_depth': (5, 12),\n",
    "         'reg_alpha': (0, 5),\n",
    "         'eta':(0.02, 0.2),\n",
    "         'reg_lambda': (0, 5)}\n",
    "    )\n",
    "    xgb_bo.maximize(init_points=21, n_iter=5)  # init_points表示初始点，n_iter代表迭代次数（即采样数）\n",
    "    print(xgb_bo.max['target'], xgb_bo.max['params'])\n",
    "    return xgb_bo.max['params']\n",
    "\n",
    "def train_predict(train, test, params):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_y = pd.read_csv(\"data/train.csv\")['target']\n",
    "    test_data = xgb.DMatrix(test)\n",
    "\n",
    "    params = params_append(params)\n",
    "    kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    prediction_test = 0\n",
    "    cv_score = []\n",
    "    prediction_train = pd.Series()\n",
    "    ESR = 30\n",
    "    NBR = 10000\n",
    "    VBE = 50\n",
    "    for train_part_index, eval_index in kf.split(train, train_y):\n",
    "\n",
    "        train_part = xgb.DMatrix(train.tocsr()[train_part_index, :],\n",
    "                                 train_y.loc[train_part_index])\n",
    "        eval = xgb.DMatrix(train.tocsr()[eval_index, :],\n",
    "                           train_y.loc[eval_index])\n",
    "        bst = xgb.train(params, train_part, NBR, [(train_part, 'train'),\n",
    "                                                          (eval, 'eval')], verbose_eval=VBE,\n",
    "                        maximize=False, early_stopping_rounds=ESR, )\n",
    "        prediction_test += bst.predict(test_data)\n",
    "        eval_pre = bst.predict(eval)\n",
    "        prediction_train = prediction_train.append(pd.Series(eval_pre, index=eval_index))\n",
    "        score = np.sqrt(mean_squared_error(train_y.loc[eval_index].values, eval_pre))\n",
    "        cv_score.append(score)\n",
    "    print(cv_score, sum(cv_score) / 5)\n",
    "    pd.Series(prediction_train.sort_index().values).to_csv(\"preprocess/train_xgboost.csv\", index=False)\n",
    "    pd.Series(prediction_test / 5).to_csv(\"preprocess/test_xgboost.csv\", index=False)\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test['target'] = prediction_test / 5\n",
    "    test[['card_id', 'target']].to_csv(\"result/submission_xgboost.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = param_beyesian(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict(train_x, test_x, best_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea075541",
   "metadata": {},
   "source": [
    "# Model ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc371c47",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f462c",
   "metadata": {},
   "source": [
    "1. mean value ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4293b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"result/submission_randomforest.csv\")\n",
    "data['randomforest'] = data['target'].values\n",
    "\n",
    "temp = pd.read_csv(\"result/submission_lightgbm.csv\")\n",
    "data['lightgbm'] = temp['target'].values\n",
    "\n",
    "\n",
    "temp = pd.read_csv(\"result/submission_xgboost.csv\")\n",
    "data['xgboost'] = temp['target'].values\n",
    "\n",
    "print(data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data['randomforest'] + data['lightgbm'] + data['xgboost']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['card_id','target']].to_csv(\"result/voting_avr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7da5e",
   "metadata": {},
   "source": [
    "2. weighing ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64309830",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = data['randomforest']*0.2+data['lightgbm']*0.3 + data['xgboost']*0.5\n",
    "data[['card_id','target']].to_csv(\"result/voting_wei1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09aac1e",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_rf  = pd.read_csv('./preprocess/train_randomforest.csv')\n",
    "predictions_rf  = pd.read_csv('./preprocess/test_randomforest.csv')\n",
    "\n",
    "oof_lgb  = pd.read_csv('./preprocess/train_lightgbm.csv')\n",
    "predictions_lgb  = pd.read_csv('./preprocess/test_lightgbm.csv')\n",
    "\n",
    "oof_xgb  = pd.read_csv('./preprocess/train_xgboost.csv')\n",
    "predictions_xgb  = pd.read_csv('./preprocess/test_xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_rf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3008be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lgb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1eabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_rf.shape, oof_lgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d27ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf.shape, predictions_lgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y):\n",
    "\n",
    "    train_stack = np.hstack([oof_1, oof_2, oof_3])\n",
    "    test_stack = np.hstack([predictions_1, predictions_2, predictions_3])\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "    \n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_+1) + \"-\" * 10)\n",
    "        clf = BayesianRidge()\n",
    "        clf.fit(trn_data, trn_y)\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_stack  = stack_model(oof_rf, oof_lgb, oof_xgb, \n",
    "                                 predictions_rf, predictions_lgb, predictions_xgb, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9158ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbd797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "666",
   "language": "python",
   "name": "666"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
